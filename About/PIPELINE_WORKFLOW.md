# MAVIS Pipeline Documentation & Roadmap

## Overview
This document outlines the end-to-end workflow for the MAVIS (Multimodal Audio-Visual Intelligent System) project. The pipeline creates cinematic video content from raw text stories by orchestrating LLM reasoning, image generation, and audio synthesis.

## 1. Project Structure
The core logic


```
phase1/
├── input/
│   └── story.txt           # Raw source text
├── output/
│   ├── event.json          # The "Blueprint" (generated by main.py)
│   ├── images/             # Generated frames
│   └── audio/              # Generated voice, sfx, and bgm
├── src/
│   ├── llm_reasoner.py     # LLM logic (Casting, Acting, Directing)
│   └── extractor.py        # NLP entity extraction
├── main.py                 # DIRECTOR ENGINE (Orchestrator)
├── generate_images.py      # VISUAL FACTORY
├── generate_audio.py       # AUDIO FACTORY
└── config.yaml             # Configuration paths & settings
```

## 2. Workflow Roadmap

### Phase 0: Input & Setup
- **Input**: The system ingests a raw text file (`input/story.txt`).
- **Config**: `config.yaml` defines model paths and output directories.

### Phase 1: The Director Engine (Orchestration)
**Script**: `python main.py`

The "Brain" of the operation. It reads the text and "hallucinates" the movie structure before assets are created.

1.  **NLP Extraction**: 
    - Identifies Scenes, Cast, and Props.
2.  **LLM Reasoning**:
    - **Casting**: Generates "Visual DNA" (consistent physical descriptions) for characters.
    - **Acting**: Assigns specific emotions (e.g., *sarcastic, terrified*) to every dialogue line.
    - **Visual Direction**: Writes strict Stable Diffusion prompts for every beat.
    - **Sound Design**: Decides on BGM styles and specific SFX placement.
3.  **Output**: `output/events.json`. This is the **Master Blueprint**.

### Phase 2: Asset Production Factories
These scripts run independently using the `events.json` blueprint.

#### A. Visual Factory
**Script**: `python generate_images.py`

- **Engine**: Stable Diffusion v1.5.
- **Process**: Iterates through beats in the blueprint and generates frames based on the "Visual DNA" prompts.
- **Output**: `output/images/{scene_id}_{beat_id}.png`

#### B. Audio Factory
**Script**: `python generate_audio.py`

- **Step 1: Casting (Parler TTS)**: Generates a unique Master Voice Reference for each character based on text descriptions.
- **Step 2: Voice Acting (XTTS v2)**: Clones the master voice to speak dialogue with specific emotions (e.g., whispering, shouting).
- **Step 3: Scoring (MusicGen)**: Composes original BGM tracks matching the scene's mood.
- **Step 4: Foley (AudioGen)**: Generates sound effects (SFX) for specific actions.
- **Output**: `output/audio/*.wav`

---

### Phase 3: Assembly (Phase 3)
*Status: Placeholder / In-Progress*

1.  **Assembly**:
    - `run_pipeline.py` currently calls a placeholder `assemble_movie()`.
    - Future: FFmpeg stitching of verified assets.

## 4. How to Run
Run the full multimodal pipeline with a single command:

```bash
# Runs Director -> Visual/Audio Factories (Parallel) -> Assembly
python run_pipeline.py
```

### Key Features
- **Parallel Execution**: Image and Audio factories run simultaneously using `ThreadPoolExecutor`.
- **Cinematic Shot Planning**: The Reasoner now assigns shot types (`CLOSE_UP`, `MEDIUM`, `WIDE`, `ESTABLISHING`).
- **Optimization**: Images are ONLY generated for relevant cinematic beats, saving computation.
- **Character Consistency**: Registry now includes reference image paths for future IP-Adapter integration.
